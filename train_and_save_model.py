# -*- coding: utf-8 -*-
"""train_and_save_model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qrZC3zJB2o_4CtOpZsuj-NDVOwhtrI41
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
# Memuat dataset
df = pd.read_csv('/content/onlinefoods.csv')

# Menampilkan informasi dasar tentang dataset
print(df.info())

# Menampilkan statistik deskriptif untuk fitur numerik
print(df.describe())

import seaborn as sns
import matplotlib.pyplot as plt
# Distribusi umur pelanggan
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], kde=True)
plt.title('Distribusi Umur Pelanggan')
plt.xlabel('Umur')
plt.ylabel('Frekuensi')
plt.show()

# Distribusi kepuasan pelanggan
plt.figure(figsize=(10, 6))
sns.countplot(x='Output', data=df)  # Menggunakan 'Output' bukan 'satisfaction'
plt.title('Distribusi Kepuasan Pelanggan')
plt.xlabel('Kepuasan')
plt.ylabel('Frekuensi')
plt.show()

# Menghapus baris dengan nilai yang hilang
df.dropna(inplace=True)

# Mendefinisikan fitur dan target
X = df.drop('Output', axis=1)  # Menggunakan 'Output' bukan 'satisfaction'
y = df['Output']

# Mengidentifikasi kolom kategorikal dan numerik
categorical_cols = X.select_dtypes(include=['object']).columns
numeric_cols = X.select_dtypes(include=['number']).columns

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
import pickle
# One-hot encoding untuk kolom kategorikal
encoder = OneHotEncoder(drop='first', sparse=False)
encoded_categorical = pd.DataFrame(encoder.fit_transform(X[categorical_cols]))
encoded_categorical.columns = encoder.get_feature_names_out(categorical_cols)
encoded_categorical.index = X.index


# Menggabungkan kembali data setelah encoding
X = X.drop(categorical_cols, axis=1)
X = pd.concat([X, encoded_categorical], axis=1)


import pandas as pd
from sklearn.preprocessing import StandardScaler
# Standard scaling untuk fitur numerik
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

from sklearn.model_selection import train_test_split

# Membagi data menjadi set pelatihan dan pengujian
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Menampilkan beberapa baris dari data pelatihan dan pengujian untuk memastikan hasilnya
print("Data Pelatihan (Fitur):")
print(X_train.head())

print("\nData Pelatihan (Target):")
print(y_train.head())

print("\nData Pengujian (Fitur):")
print(X_test.head())

print("\nData Pengujian (Target):")
print(y_test.head())

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Fungsi untuk mengevaluasi model
def evaluate_model(y_true, y_pred, model_name):
    print(f"Evaluasi {model_name}:")
    print("Akurasi:", accuracy_score(y_true, y_pred))
    print("Presisi:", precision_score(y_true, y_pred, average='macro'))
    print("Recall:", recall_score(y_true, y_pred, average='macro'))
    print("F1-Score:", f1_score(y_true, y_pred, average='macro'))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("Classification Report:\n", classification_report(y_true, y_pred))

# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
evaluate_model(y_test, y_pred_log_reg, "Logistic Regression")

# Decision Tree
dec_tree = DecisionTreeClassifier()
dec_tree.fit(X_train, y_train)
y_pred_dec_tree = dec_tree.predict(X_test)
evaluate_model(y_test, y_pred_dec_tree, "Decision Tree")

# Fungsi untuk mengevaluasi model
def evaluate_model(y_true, y_pred, model_name):
    print(f"Evaluasi {model_name}:")
    print("Akurasi:", accuracy_score(y_true, y_pred))
    print("Presisi:", precision_score(y_true, y_pred, average='macro'))
    print("Recall:", recall_score(y_true, y_pred, average='macro'))
    print("F1-Score:", f1_score(y_true, y_pred, average='macro'))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("Classification Report:\n", classification_report(y_true, y_pred))

    # K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
evaluate_model(y_test, y_pred_knn, "K-Nearest Neighbors")

# Fungsi untuk mengevaluasi model
def evaluate_model(y_true, y_pred, model_name):
    print(f"Evaluasi {model_name}:")
    print("Akurasi:", accuracy_score(y_true, y_pred))
    print("Presisi:", precision_score(y_true, y_pred, average='macro'))
    print("Recall:", recall_score(y_true, y_pred, average='macro'))
    print("F1-Score:", f1_score(y_true, y_pred, average='macro'))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("Classification Report:\n", classification_report(y_true, y_pred))

    # Hyperparameter Tuning for Logistic Regression
from sklearn.model_selection import GridSearchCV, cross_val_score
param_grid_log_reg = {'C': [0.01, 0.1, 1, 10, 100]}
grid_log_reg = GridSearchCV(LogisticRegression(), param_grid_log_reg, cv=5)
grid_log_reg.fit(X_train, y_train)
best_log_reg = grid_log_reg.best_estimator_
print("Best parameters for Logistic Regression:", grid_log_reg.best_params_)

# Hyperparameter Tuning for Decision Tree
param_grid_dec_tree = {'max_depth': [3, 5, 7, 10], 'min_samples_split': [2, 5, 10]}
grid_dec_tree = GridSearchCV(DecisionTreeClassifier(), param_grid_dec_tree, cv=5)
grid_dec_tree.fit(X_train, y_train)
best_dec_tree = grid_dec_tree.best_estimator_
print("Best parameters for Decision Tree:", grid_dec_tree.best_params_)

# Hyperparameter Tuning for K-Nearest Neighbors
param_grid_knn = {'n_neighbors': [3, 5, 7, 10], 'weights': ['uniform', 'distance']}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_knn.fit(X_train, y_train)
best_knn = grid_knn.best_estimator_
print("Best parameters for K-Nearest Neighbors:", grid_knn.best_params_)

# Cross-Validation for Logistic Regression
cv_log_reg = cross_val_score(best_log_reg, X_train, y_train, cv=5)
print("Cross-Validation Scores for Logistic Regression:", cv_log_reg)
print("Mean Cross-Validation Score for Logistic Regression:", cv_log_reg.mean())

# Cross-Validation for Decision Tree
cv_dec_tree = cross_val_score(best_dec_tree, X_train, y_train, cv=5)
print("Cross-Validation Scores for Decision Tree:", cv_dec_tree)
print("Mean Cross-Validation Score for Decision Tree:", cv_dec_tree.mean())

# Cross-Validation for K-Nearest Neighbors
cv_knn = cross_val_score(best_knn, X_train, y_train, cv=5)
print("Cross-Validation Scores for K-Nearest Neighbors:", cv_knn)
print("Mean Cross-Validation Score for K-Nearest Neighbors:", cv_knn.mean())

# Visualizing Model Performance
models = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbors']
accuracies = [accuracy_score(y_test, y_pred_log_reg), accuracy_score(y_test, y_pred_dec_tree), accuracy_score(y_test, y_pred_knn)]
precisions = [precision_score(y_test, y_pred_log_reg, average='macro'), precision_score(y_test, y_pred_dec_tree, average='macro'), precision_score(y_test, y_pred_knn, average='macro')]
recalls = [recall_score(y_test, y_pred_log_reg, average='macro'), recall_score(y_test, y_pred_dec_tree, average='macro'), recall_score(y_test, y_pred_knn, average='macro')]
f1_scores = [f1_score(y_test, y_pred_log_reg, average='macro'), f1_score(y_test, y_pred_dec_tree, average='macro'), f1_score(y_test, y_pred_knn, average='macro')]

plt.figure(figsize=(14, 8))

plt.subplot(2, 2, 1)
sns.barplot(x=models, y=accuracies)
plt.title('Model Accuracy')

plt.subplot(2, 2, 2)
sns.barplot(x=models, y=precisions)
plt.title('Model Precision')

plt.subplot(2, 2, 3)
sns.barplot(x=models, y=recalls)
plt.title('Model Recall')

plt.subplot(2, 2, 4)
sns.barplot(x=models, y=f1_scores)
plt.title('Model F1-Score')

plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Pelatihan model Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Evaluasi model Random Forest
evaluate_model(y_test, y_pred_rf, "Random Forest")

# Validasi Silang untuk Model Random Forest
cv_scores_rf = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')
print("Validasi Silang Akurasi Random Forest:", cv_scores_rf)

from sklearn.model_selection import  cross_val_score

# Inisialisasi variabel untuk menyimpan hasil validasi silang
cv_means = []
cv_stds = []
model_names = []

# Pelatihan dan validasi silang untuk Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
cv_scores_log_reg = cross_val_score(log_reg, X, y, cv=5, scoring='accuracy')
cv_means.append(cv_scores_log_reg.mean())
cv_stds.append(cv_scores_log_reg.std())
model_names.append("Logistic Regression")

# Pelatihan dan validasi silang untuk Decision Tree
dec_tree = DecisionTreeClassifier()
dec_tree.fit(X_train, y_train)
cv_scores_dec_tree = cross_val_score(dec_tree, X, y, cv=5, scoring='accuracy')
cv_means.append(cv_scores_dec_tree.mean())
cv_stds.append(cv_scores_dec_tree.std())
model_names.append("Decision Tree")

# Pelatihan dan validasi silang untuk K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
cv_scores_knn = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
cv_means.append(cv_scores_knn.mean())
cv_stds.append(cv_scores_knn.std())
model_names.append("K-Nearest Neighbors")

# Pelatihan dan validasi silang untuk Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
cv_scores_rf = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')
cv_means.append(cv_scores_rf.mean())
cv_stds.append(cv_scores_rf.std())
model_names.append("Random Forest")

# Visualisasi Hasil Validasi Silang
plt.figure(figsize=(10, 5))
plt.bar(model_names, cv_means, yerr=cv_stds)
plt.title('Perbandingan Validasi Silang Akurasi Model')
plt.xlabel('Model')
plt.ylabel('Rata-rata Akurasi')
plt.show()

import pickle

# Save the model, encoder, and scaler
with open('model.pkl', 'wb') as f:
    pickle.dump(best_log_reg, f)

with open('encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

import pickle

# Misal model Anda adalah log_reg yang sudah terlatih
with open('model.pkl', 'wb') as file:
    pickle.dump(log_reg, file)

import pickle

# Misal model Anda adalah log_reg yang sudah terlatih
with open('model.pkl', 'wb') as file:
    pickle.dump(log_reg, file)

pip install streamlit

import streamlit as st
import pickle
import numpy as np

# Load model
model = pickle.load(open('model.pkl', 'rb'))

st.title('Prediksi Kepuasan Pelanggan')

# Input dari pengguna
age = st.number_input('Umur', min_value=0, max_value=100, step=1)
gender = st.selectbox('Jenis Kelamin', ('Pria', 'Wanita'))
income = st.number_input('Pendapatan Tahunan', min_value=0)
score = st.number_input('Skor Pengeluaran', min_value=0, max_value=100, step=1)

# Konversi input ke dalam bentuk yang sesuai untuk model
gender_encoded = 1 if gender == 'Pria' else 0
features = np.array([[age, gender_encoded, income, score]])

if st.button('Prediksi'):
    prediction = model.predict(features)
    output = 'Puas' if prediction[0] == 1 else 'Tidak Puas'
    st.write(f'Prediksi Kepuasan Pelanggan: {output}')